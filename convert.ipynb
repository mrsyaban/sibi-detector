{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export .pt to .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.76  Python-3.10.10 torch-2.4.0+cu124 CPU (AMD Ryzen 7 4800H with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11,135,646 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\v7\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 30, 8400) (21.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.1 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  6.7s, saved as 'models\\v7\\best.onnx' (42.7 MB)\n",
      "\n",
      "Export complete (16.0s)\n",
      "Results saved to \u001b[1mD:\\myres-2024\\Sign-Language-Alphabets-Detection-and-Recongition-using-YOLOv8\\models\\v7\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\v7\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=models\\v7\\best.onnx imgsz=640 data=D:\\myres-2024\\Sign-Language-Alphabets-Detection-and-Recongition-using-YOLOv8\\datasets\\American-Sign-Language-Letters-1\\data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models\\\\v7\\\\best.onnx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"models/v7/best.pt\")\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(format=\"onnx\")  # creates 'yolov8n.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX to ORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.tools.convert_onnx_models_to_ort import convert_onnx_models_to_ort\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = Path(\"models/v4/best.onnx\")\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Convert to ORT format\n",
    "ort_model_path = Path(\"models/v4/best.ort\")\n",
    "convert_onnx_models_to_ort(onnx_model_path, ort_model_path.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil dikompresi.\n"
     ]
    }
   ],
   "source": [
    "# import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Load model ONNX\n",
    "model_fp32 = 'models/v4/best.onnx'\n",
    "model_quant = 'models/v4/best_compressed.onnx'\n",
    "\n",
    "# Lakukan quantization\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)\n",
    "\n",
    "print(\"Model berhasil dikompresi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
